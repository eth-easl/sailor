{
	"global_batch_size": 1024,
	"type": "gpt2",
	"hidden_size": 1024,
	"sequence_length": 2048,
	"num_layers": 24,
	"vocab_size": 50272,
	"model": "OPT-350",
	"optimizer": "Adam",
    "heads": 16,
    "head_dim": 64,
	"max_position_embeddings": 2048,
	"num_all_layers": 26
}
