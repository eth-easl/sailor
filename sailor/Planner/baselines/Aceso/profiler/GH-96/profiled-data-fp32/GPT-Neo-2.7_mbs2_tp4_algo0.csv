op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,603.694,972.110,0.031,40.000,333.125,50.035,119.965,522.000
enc-1st-layernorm,37.074,118.256,40.000,80.000,0.000,40.031,0.000,160.000
enc-attention-qkv,181.413,656.575,80.000,70.000,18.750,30.000,32.000,70.000
enc-attention-score,108.325,225.914,70.000,306.000,0.000,256.000,256.000,30.000
enc-attention-softmax,763.762,1045.042,306.000,306.000,0.000,256.000,256.000,512.000
enc-attention-dropout,209.057,191.820,306.000,306.000,0.000,320.000,0.000,512.000
enc-attention-context,111.783,203.419,306.000,50.000,0.000,10.000,10.000,266.000
enc-attention-dense,452.787,106.764,50.000,80.010,6.250,40.000,0.000,50.000
enc-post-attention-dropout,110.614,68.820,80.010,40.000,0.000,50.000,40.000,160.000
enc-2nd-layernorm,36.961,117.826,40.000,80.000,0.000,40.031,0.000,160.000
enc-MLP-GEMM-1,169.301,630.641,80.000,80.010,25.000,40.000,0.000,80.000
enc-MLP-gelu,31.900,100.315,80.010,80.000,0.000,40.000,0.000,160.000
enc-MLP-GEMM-2,499.088,161.886,80.000,80.010,25.000,40.000,0.000,80.000
enc-post-MLP-dropout,110.191,70.572,80.010,40.000,0.000,50.000,40.000,160.000
final-layernorm,73.385,333.577,40.000,40.000,0.000,80.031,0.000,80.000
gpt-post-process,3588.259,2716.148,40.000,0.000,313.125,502.051,0.000,0.000
