op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,503.427,724.179,0.016,20.000,166.562,30.035,59.965,286.000
enc-1st-layernorm,45.246,95.391,20.000,40.000,0.000,20.031,0.000,160.000
enc-attention-qkv,720.125,139.463,25.000,80.000,9.375,60.000,60.000,180.000
enc-attention-score,78.815,144.136,50.000,168.000,0.000,128.000,128.000,276.000
enc-attention-softmax,91.743,145.966,168.000,168.000,0.000,128.000,0.000,384.000
enc-attention-dropout,149.626,98.157,168.000,168.000,0.000,192.000,0.000,512.000
enc-attention-context,90.885,122.184,168.000,40.000,0.000,5.000,0.000,138.000
enc-attention-dense,367.361,93.842,40.000,55.005,3.125,20.000,0.000,60.000
enc-post-attention-dropout,87.476,65.619,55.005,35.000,0.000,30.000,20.000,160.000
enc-2nd-layernorm,42.546,92.679,35.000,55.000,0.000,20.031,0.000,160.000
enc-MLP-GEMM-1,85.628,392.616,55.000,55.005,12.500,20.000,0.000,80.000
enc-MLP-gelu,40.185,97.615,55.005,55.000,0.000,20.000,0.000,160.000
enc-MLP-GEMM-2,346.869,95.052,55.000,55.005,12.500,20.000,0.000,80.000
enc-post-MLP-dropout,90.116,67.991,55.005,35.000,0.000,30.000,20.000,160.000
final-layernorm,67.931,324.744,35.000,35.000,0.000,40.031,0.000,80.000
gpt-post-process,3045.291,2104.443,35.000,15.000,156.562,502.051,251.949,0.000
