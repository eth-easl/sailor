op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,589.222,1062.030,0.008,28.000,378.000,42.018,83.982,462.000
enc-1st-layernorm,50.110,126.135,28.000,56.000,0.000,28.016,0.000,224.000
enc-attention-qkv,1619.476,544.900,42.000,112.000,147.000,84.000,84.000,224.000
enc-attention-score,110.739,194.097,84.000,280.000,0.000,224.000,224.000,490.000
enc-attention-softmax,154.662,247.973,280.000,280.000,0.000,224.000,0.000,672.000
enc-attention-dropout,257.188,168.091,280.000,280.000,0.000,336.000,0.000,896.000
enc-attention-context,116.646,200.480,280.000,56.000,0.000,14.000,14.000,266.000
enc-attention-dense,567.561,142.324,56.000,70.014,49.000,28.000,0.000,98.000
enc-post-attention-dropout,104.678,71.770,70.014,42.000,0.000,42.000,14.000,224.000
enc-2nd-layernorm,50.515,127.840,42.000,70.000,0.000,28.016,0.000,224.000
enc-MLP-GEMM-1,647.175,1192.909,70.000,98.027,196.000,56.000,0.000,196.000
enc-MLP-gelu,58.979,101.054,98.027,98.000,0.000,56.000,0.000,358.000
enc-MLP-GEMM-2,1085.269,598.156,98.000,70.014,196.000,28.000,0.000,140.000
enc-post-MLP-dropout,117.767,71.728,70.014,42.000,0.000,42.000,14.000,224.000
final-layernorm,53.275,223.488,42.000,42.000,0.000,28.016,0.000,112.000
gpt-post-process,1939.005,1774.418,42.000,14.000,350.000,200.025,99.975,0.000
