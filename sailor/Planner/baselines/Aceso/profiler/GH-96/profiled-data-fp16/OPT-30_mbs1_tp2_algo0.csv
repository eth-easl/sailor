op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,589.222,1062.030,0.008,28.000,378.000,42.018,83.982,462.000
enc-1st-layernorm,50.110,126.135,28.000,56.000,0.000,28.016,0.000,224.000
enc-attention-qkv,537.550,1002.699,56.000,70.000,147.000,42.000,32.000,154.000
enc-attention-score,110.739,194.097,70.000,266.000,0.000,224.000,224.000,490.000
enc-attention-softmax,154.662,247.973,266.000,266.000,0.000,224.000,0.000,672.000
enc-attention-dropout,257.188,168.091,266.000,266.000,0.000,336.000,0.000,896.000
enc-attention-context,116.646,200.480,266.000,42.000,0.000,14.000,14.000,266.000
enc-attention-dense,563.550,139.523,42.000,56.014,49.000,28.000,0.000,0.000
enc-post-attention-dropout,104.678,71.770,56.014,28.000,0.000,42.000,14.000,224.000
enc-2nd-layernorm,50.515,127.840,28.000,56.000,0.000,28.016,0.000,224.000
enc-MLP-GEMM-1,621.355,1223.391,56.000,84.027,196.000,56.000,0.000,196.000
enc-MLP-gelu,58.979,101.054,84.027,84.000,0.000,56.000,0.000,358.000
enc-MLP-GEMM-2,1057.643,620.413,84.000,56.014,196.000,28.000,0.000,140.000
enc-post-MLP-dropout,117.767,71.728,56.014,28.000,0.000,42.000,14.000,224.000
final-layernorm,53.275,223.488,28.000,28.000,0.000,28.016,0.000,112.000
gpt-post-process,1939.005,1774.418,28.000,0.000,350.000,200.025,99.975,0.000
