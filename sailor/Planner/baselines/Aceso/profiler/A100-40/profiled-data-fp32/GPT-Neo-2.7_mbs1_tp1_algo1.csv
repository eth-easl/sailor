op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,224.382,4275.376,0.016,20.000,1272.500,25.000,75.000,1254.000
enc-1st-layernorm,59.032,170.571,20.000,40.000,0.000,20.016,0.000,80.000
enc-attention-qkv,853.419,691.581,40.000,80.000,75.000,60.000,0.000,0.000
enc-attention-score,490.338,1010.180,80.000,552.000,0.000,512.000,512.000,60.000
enc-attention-softmax,2922.159,4760.820,552.000,552.000,0.000,512.000,512.000,1024.000
enc-attention-dropout,1013.559,881.284,552.000,552.000,0.000,640.000,0.000,1024.000
enc-attention-context,558.448,965.732,552.000,40.000,0.000,20.000,20.000,532.000
enc-attention-dense,216.889,217.545,40.000,40.010,25.000,20.000,0.000,0.000
enc-post-attention-dropout,121.760,100.058,40.010,20.000,0.000,25.000,15.000,80.000
enc-2nd-layernorm,59.944,159.210,20.000,40.000,0.000,20.016,0.000,80.000
enc-MLP-GEMM-1,829.041,840.575,40.000,100.039,100.000,80.000,0.000,0.000
enc-MLP-gelu,139.970,274.128,100.039,100.000,0.000,80.000,0.000,270.000
enc-MLP-GEMM-2,840.819,853.598,100.000,40.010,100.000,20.000,0.000,0.000
enc-post-MLP-dropout,123.894,97.233,40.010,20.000,0.000,25.000,15.000,80.000
final-layernorm,61.792,406.623,20.000,20.000,0.000,20.016,0.000,40.000
gpt-post-process,15907.228,11003.715,20.000,0.000,1252.500,1002.025,0.000,0.000
